{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método build_features('SamplesPath.txt', arraywords(Y)) \n",
    "\n",
    "Parametros: un array con las palabra y el path del archivo que contiene las rutas de los audios analizar\n",
    "\n",
    "Retorna: Matriz con #filas: cantidad de audios, #columna: cantidad de bloques por audio +2 columnas adicionales donde en la primera es la clase a la que pertenece y la segunda es el id del hablante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading https://files.pythonhosted.org/packages/31/42/4c0bbb66390e3a68e04ebf134c8d074a00c18b5882293f8ace5f7497fbf0/ipynb-0.5.1-py3-none-any.whl\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(PATHSAMPLES, SOUNDS):\n",
    "    #matrixFeatures = np.zeros((,5))\n",
    "    data = pd.read_csv(PATHSAMPLES, sep=\" \", header=None)\n",
    "    data = data.values #Convertir en numpy array\n",
    "    labels = []\n",
    "    speakers  = []\n",
    "    paths = []\n",
    "    DIVISIONROW, DIVISIONCOL = 9,7\n",
    "    for path in data:\n",
    "        pathSplit =path[0].split('/')\n",
    "        speakerName = pathSplit[1].split('_')[0]\n",
    "        word = pathSplit[0]\n",
    "        isWord = np.where(SOUNDS == word)\n",
    "        if len(isWord[0]) != 0:\n",
    "            label = SOUNDS[isWord[0][0]]\n",
    "            labels.append(label)\n",
    "            speakers.append(speakerName)\n",
    "            paths.append(path[0])\n",
    "    \n",
    "    #bloques y ademas una columna para la etiqueta o sonido que represente y otra para el id del hablante\n",
    "    #NUMERO DE MUESTRAS POR CLASE\n",
    "    NSAMPLES = 1300\n",
    "    classesMatrix = []\n",
    "    for i in SOUNDS:\n",
    "        first = labels.index(i)\n",
    "        last = first +NSAMPLES\n",
    "        clasei = np.column_stack([paths[first:last], labels[first:last], speakers[first:last]])    \n",
    "        classesMatrix.append(clasei)\n",
    "    class1 = classesMatrix[0]\n",
    "    class2 = classesMatrix[1]\n",
    "    class3 = classesMatrix[2]\n",
    "    class4 = classesMatrix[3]   \n",
    "    # Contienes todos los paths , palabra y idSpeaker de 1300 por clase\n",
    "    matrixMajor = np.vstack((class1,class2,class3,class4))\n",
    "    rows = matrixMajor.shape[0] # numero de filas es la cantidad de audios \n",
    "    colums = DIVISIONROW*DIVISIONCOL*2+2 # cantidad de bloques por 2 ya que esta la media y std de todos los \n",
    "    matrixFeatures = np.zeros((rows,colums)) # MATRIZ se guardara como csv para el entrenamiento\n",
    "    \n",
    "    #Recorre la matriz, importar el metodo get_features_audio del archivo get_features_audio, extraer las 5200 features, codificar la columna de label y guardad en un archivo\n",
    "    #for infoSound in matrixMajor:\n",
    "         \n",
    "    \n",
    "    \n",
    "    #Seleccióno la cantidad de audios por clase\n",
    "    #get_features_audio(PATHTEST, DIVISIONROW, DIVISIONCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHSAMPLES = './training_list.txt'\n",
    "SOUNDS =np.array(['backward','bed','cat','eight'])\n",
    "\n",
    "build_features(PATHSAMPLES,SOUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.where(SOUNDS == 'cat')\n",
    "print(s[0][0])\n",
    "SOUNDS[s[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_features_audio import GETFEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GF = GETFEATURES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readAudios' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6dd7f759be86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features_audios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward/017c4098_nohash_3.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Udea/2019-1/Semillero/Voice_Recognition/get_features_audio.py\u001b[0m in \u001b[0;36mget_features_audios\u001b[0;34m(self, path, num_rows, num_columns)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_features_audios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadAudios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_specgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readAudios' is not defined"
     ]
    }
   ],
   "source": [
    "f=GF.get_features_audios('backward/017c4098_nohash_3.wav', 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
